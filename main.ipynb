{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNAn2tM6rVVPIq26I2OzqWi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# !pip install wandb\n","\n","# import wandb\n","# import os\n","# wandb.login(key=\"2ba3e7324f00984d9cc7150478390d2c569ba8cf\")\n","\n","# wandb.init(project=\"Methembe-GNN5\", name='ST-GAT_model')"],"metadata":{"id":"D6kfJJxvEatf","executionInfo":{"status":"ok","timestamp":1719245116255,"user_tz":-120,"elapsed":7,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","%cd /content/drive/MyDrive/ST-GAT\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RVwzl6lyWxr","executionInfo":{"status":"ok","timestamp":1719245148437,"user_tz":-120,"elapsed":32187,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}},"outputId":"7ba7a12a-b849-4d14-914c-f9ff3ee9f597"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ST-GAT\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NpeQUqWgw_GI","executionInfo":{"status":"ok","timestamp":1719245170321,"user_tz":-120,"elapsed":21890,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e01218a-5dd6-469f-8d97-4b900a2bfbb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.6.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.5.3\n"]}],"source":["!pip install torch_geometric\n","\n","import random\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import torch_geometric\n","from torch_geometric.nn.conv import GATConv\n","import torch.nn.functional as F\n","from load_data import *\n","from utils import *\n","from stgat import *\n","\n","torch.manual_seed(2333)\n","torch.cuda.manual_seed(2333)\n","np.random.seed(2333)\n","random.seed(2333)\n","torch.backends.cudnn.deterministic = True\n","\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","\n","matrix_path = \"./dataset/03062021_M_96.csv\"\n","data_path = \"./dataset/03062021_T_adj_96.csv\"\n","save_path = \"./save/GAT_model_2.pt\"\n","\n","day_slot = 288\n","n_train, n_val, n_test = 69, 15, 15\n","\n","n_his = 24\n","n_pred = 12\n","n_route = 96\n","Ks, Kt = 3, 3\n","blocks = [[1, 32, 64], [64, 32, 128]]\n","drop_prob = 0.31\n","\n","batch_size = 147\n","epochs = 100\n","lr = 0.0503\n","weight_decay = 0.03739\n","\n","train, val, test = load_data(data_path, n_train * day_slot, n_val * day_slot)\n","scaler = StandardScaler()\n","train = scaler.fit_transform(train)\n","val = scaler.transform(val)\n","test = scaler.transform(test)\n","\n","x_train, y_train = data_transform(train, n_his, n_pred, day_slot, device)\n","x_val, y_val = data_transform(val, n_his, n_pred, day_slot, device)\n","x_test, y_test = data_transform(test, n_his, n_pred, day_slot, device)\n","\n","train_data = torch.utils.data.TensorDataset(x_train, y_train)\n","train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n","val_data = torch.utils.data.TensorDataset(x_val, y_val)\n","val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n","test_data = torch.utils.data.TensorDataset(x_test, y_test)\n","test_iter = torch.utils.data.DataLoader(test_data, batch_size)\n"]},{"cell_type":"code","source":["def calculate_edge_index_from_adj_matrix(csv_file):\n","  \"\"\"\n","  Calculates the edge index from an adjacency matrix in a CSV file.\n","\n","  Args:\n","      csv_file (str): Path to the CSV file containing the adjacency matrix.\n","\n","  Returns:\n","      numpy.ndarray: A 2D NumPy array of shape (num_edges, 2) representing the edge index.\n","                        - Each row contains [source_node_index, target_node_index] for an edge.\n","  \"\"\"\n","  # Read the adjacency matrix\n","  adj_matrix = pd.read_csv(csv_file, header=None).values\n","\n","  # Find non-zero elements (assuming they represent edges)\n","  edges = adj_matrix.nonzero()\n","\n","  # Combine source and target node indices into a single 2D array\n","  edge_index = np.vstack([edges[0], edges[1]])\n","\n","  edge_index_tensor = torch.LongTensor(edge_index)\n","\n","  return edge_index_tensor\n","\n","# Example usage\n","# csv_file = \"./dataset/03062021_M_96.csv\"\n","edge_index = calculate_edge_index_from_adj_matrix(matrix_path)\n","\n","edge_index = edge_index.to(device)\n","\n","print(\"Edge Index:\", edge_index.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAu7D1cPxvyX","executionInfo":{"status":"ok","timestamp":1719245171000,"user_tz":-120,"elapsed":684,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}},"outputId":"f3b079c7-3638-4efe-d117-5ad74a8fcc93"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Edge Index: torch.Size([2, 8624])\n"]}]},{"cell_type":"code","source":["# Example usage\n","in_features = 96  # Assuming your nodes have 10 features\n","hidden_channels = 96\n","num_layers = 2\n","out_features = 96  # Number of output features for each node after prediction\n","\n","\n","\n","model = BasicGAT(in_features, hidden_channels, num_layers, out_features)\n","model = model.to(device)\n","loss = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n"],"metadata":{"id":"wYfxUr0vKqSH","executionInfo":{"status":"ok","timestamp":1719245171000,"user_tz":-120,"elapsed":3,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, epochs + 1):\n","    model.train()\n","    final_loss ,train_loss , n , test_n = 0.0 , 0.0 , 0 , 0\n","    mape_batch , mae_batch , rmse_batch = 0.0 , 0.0 , 0.0\n","\n","    mape_test_batch , mae_test_batch , rmse_test_batch = 0.0 , 0.0 , 0.0\n","    test_loss = 0.0\n","\n","\n","    for x, y in train_iter:\n","        # print(x.size())\n","        y_pred = model(x , edge_index)\n","        loss_value = loss(y_pred, y)\n","        loss_value.backward()\n","        optimizer.zero_grad()\n","        optimizer.step()\n","        train_loss += loss_value.item()\n","        n += y.shape[0]\n","\n","        x_std = torch.std(x)\n","        x_mean = torch.mean(x)\n","        y = z_inverse(y , x_mean , x_std)\n","        y_pred = z_inverse(y_pred , x_mean , x_std)\n","\n","        mape_batch += MAPE(y, y_pred)\n","        mae_batch += MAE(y, y_pred)\n","        rmse_batch += RMSE(y, y_pred)\n","\n","    for x, y in test_iter:\n","        y_pred = model(x , edge_index)\n","        test_loss = loss(y_pred, y)\n","        test_loss.backward()\n","        optimizer.zero_grad()\n","        optimizer.step()\n","\n","        y_pred = z_inverse(y_pred , x_mean , x_std)\n","        y = z_inverse(y , x_mean , x_std)\n","\n","        test_n += y.shape[0]\n","\n","        mape_test_batch += MAPE(y, y_pred)\n","        mae_test_batch += MAE(y, y_pred)\n","        rmse_test_batch += RMSE(y, y_pred)\n","\n","    scheduler.step()\n","\n","    # Calculate MAPE, MAE, RMSE after testing each epoch\n","    mape_test = mape_test_batch/test_n\n","    mae_test = mae_test_batch/test_n\n","    rmse_test = rmse_test_batch/test_n\n","    test_loss = test_loss/test_n\n","\n","    # Calculate MAPE, MAE, RMSE after training each epoch\n","    mape_ = mape_batch/n\n","    mae_ = mae_batch/n\n","    rmse_ = rmse_batch/n\n","    final_loss = train_loss/n\n","\n","    # wandb.log({\"Training_Loss\": loss_value.item() , \"Testing_Loss\": test_loss.item()  ,\"Test_RMSE\": rmse_test ,\"Test_MAE\": mae_test , \"Test_MAPE\":mape_test ,\n","    #          \"RMSE\": rmse_, \"MAE\": mae_ , \"MAPE\": mape_*100 }, step=epoch)\n","\n","    if epoch % 5 == 0:\n","        torch.save(model.state_dict(), save_path)\n","\n","    print(f\"Epoch number: {epoch} - Training Loss: {final_loss} - MAE: {mae_} - MAPE: {mape_*100} - RMSE: {rmse_} \")\n"],"metadata":{"id":"IpPkH_944JfL","executionInfo":{"status":"ok","timestamp":1719245290772,"user_tz":-120,"elapsed":119775,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b65b1f0e-a805-4e0b-cee2-8c2e97fdb189"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n","/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch number: 1 - Training Loss: 0.010929209209358446 - MAE: 0.006650480907410383 - MAPE: -0.3816549777984619 - RMSE: 0.009119877591729164 \n","Epoch number: 2 - Training Loss: 0.010928010936454515 - MAE: 0.006653799209743738 - MAPE: -0.34926339983940125 - RMSE: 0.009121833369135857 \n","Epoch number: 3 - Training Loss: 0.010930932735177177 - MAE: 0.006655038800090551 - MAPE: -0.28191566467285156 - RMSE: 0.009124530479311943 \n","Epoch number: 4 - Training Loss: 0.010928663982383272 - MAE: 0.006653320975601673 - MAPE: 0.20068466663360596 - RMSE: 0.009120623581111431 \n","Epoch number: 5 - Training Loss: 0.010925656906373109 - MAE: 0.0066506871953606606 - MAPE: -1.777777075767517 - RMSE: 0.009118027985095978 \n","Epoch number: 6 - Training Loss: 0.010927309645487625 - MAE: 0.006652703508734703 - MAPE: -0.25022804737091064 - RMSE: 0.009121185168623924 \n","Epoch number: 7 - Training Loss: 0.01092936779998344 - MAE: 0.0066557084210217 - MAPE: -0.3656529486179352 - RMSE: 0.009122596122324467 \n","Epoch number: 8 - Training Loss: 0.01093506747493426 - MAE: 0.00665120966732502 - MAPE: -0.513572096824646 - RMSE: 0.009122251532971859 \n","Epoch number: 9 - Training Loss: 0.010933570253512373 - MAE: 0.006652661133557558 - MAPE: -2.9322705268859863 - RMSE: 0.009122801944613457 \n","Epoch number: 10 - Training Loss: 0.010935588719379509 - MAE: 0.006654052063822746 - MAPE: 0.037058115005493164 - RMSE: 0.009124241769313812 \n","Epoch number: 11 - Training Loss: 0.010923377944615013 - MAE: 0.006650167983025312 - MAPE: -1.2816308736801147 - RMSE: 0.009118041023612022 \n","Epoch number: 12 - Training Loss: 0.01092884297727917 - MAE: 0.00665273517370224 - MAPE: 0.07644988596439362 - RMSE: 0.009118426591157913 \n","Epoch number: 13 - Training Loss: 0.010928269411038853 - MAE: 0.006649642251431942 - MAPE: -1.318831205368042 - RMSE: 0.009116834960877895 \n","Epoch number: 14 - Training Loss: 0.010929960636924058 - MAE: 0.006654791533946991 - MAPE: 0.5894120931625366 - RMSE: 0.00912450160831213 \n","Epoch number: 15 - Training Loss: 0.010925635948974378 - MAE: 0.006652771029621363 - MAPE: -0.11359252035617828 - RMSE: 0.009120739065110683 \n","Epoch number: 16 - Training Loss: 0.010933366620527645 - MAE: 0.00665243761613965 - MAPE: -2.2148823738098145 - RMSE: 0.009120522066950798 \n","Epoch number: 17 - Training Loss: 0.010928075420233012 - MAE: 0.006651524920016527 - MAPE: -4.239975929260254 - RMSE: 0.009121662005782127 \n","Epoch number: 18 - Training Loss: 0.010937837190819803 - MAE: 0.006657744757831097 - MAPE: -0.6989822387695312 - RMSE: 0.009126379154622555 \n","Epoch number: 19 - Training Loss: 0.010928954586182671 - MAE: 0.00665131164714694 - MAPE: -0.4528490900993347 - RMSE: 0.009118815883994102 \n","Epoch number: 20 - Training Loss: 0.010929746569626765 - MAE: 0.00665723392739892 - MAPE: 4.8971405029296875 - RMSE: 0.009123716503381729 \n","Epoch number: 21 - Training Loss: 0.010932418190682474 - MAE: 0.006652954965829849 - MAPE: -1.2548936605453491 - RMSE: 0.009122121147811413 \n","Epoch number: 22 - Training Loss: 0.010927650044448817 - MAE: 0.006654639728367329 - MAPE: -0.7762792706489563 - RMSE: 0.009123084135353565 \n","Epoch number: 23 - Training Loss: 0.010923779460786579 - MAE: 0.006651489529758692 - MAPE: -0.919911801815033 - RMSE: 0.009119112975895405 \n","Epoch number: 24 - Training Loss: 0.010931649035711486 - MAE: 0.00665148114785552 - MAPE: -1.546188473701477 - RMSE: 0.009122682735323906 \n","Epoch number: 25 - Training Loss: 0.010930758206276316 - MAE: 0.006653919816017151 - MAPE: -1.1439440250396729 - RMSE: 0.009122363291680813 \n","Epoch number: 26 - Training Loss: 0.01092860344561568 - MAE: 0.006655137054622173 - MAPE: -0.31935837864875793 - RMSE: 0.009118648245930672 \n","Epoch number: 27 - Training Loss: 0.010932361744329296 - MAE: 0.0066514648497104645 - MAPE: -1220.26220703125 - RMSE: 0.009119514375925064 \n","Epoch number: 28 - Training Loss: 0.010929184530297184 - MAE: 0.00665078591555357 - MAPE: -1.4715018272399902 - RMSE: 0.00911904126405716 \n","Epoch number: 29 - Training Loss: 0.01093475629614549 - MAE: 0.006653661839663982 - MAPE: -0.9246572852134705 - RMSE: 0.009124986827373505 \n","Epoch number: 30 - Training Loss: 0.01092731058785355 - MAE: 0.0066528478637337685 - MAPE: -0.780755877494812 - RMSE: 0.009119205176830292 \n","Epoch number: 31 - Training Loss: 0.010932991613519282 - MAE: 0.006651902571320534 - MAPE: -0.001043513766489923 - RMSE: 0.009120635688304901 \n","Epoch number: 32 - Training Loss: 0.010925146615224557 - MAE: 0.00665252935141325 - MAPE: -0.30735090374946594 - RMSE: 0.009118449874222279 \n","Epoch number: 33 - Training Loss: 0.010927428561141416 - MAE: 0.006654583849012852 - MAPE: -2.771289348602295 - RMSE: 0.009123332798480988 \n","Epoch number: 34 - Training Loss: 0.010933723927445583 - MAE: 0.0066519021056592464 - MAPE: -0.7573750615119934 - RMSE: 0.009122668765485287 \n","Epoch number: 35 - Training Loss: 0.01092802625331517 - MAE: 0.006652784999459982 - MAPE: -0.48695722222328186 - RMSE: 0.009120771661400795 \n","Epoch number: 36 - Training Loss: 0.010930330228742718 - MAE: 0.006651738192886114 - MAPE: -0.7994388937950134 - RMSE: 0.009119558148086071 \n","Epoch number: 37 - Training Loss: 0.010936941690527438 - MAE: 0.006656268145889044 - MAPE: -0.31648561358451843 - RMSE: 0.009124288335442543 \n","Epoch number: 38 - Training Loss: 0.010928377817263956 - MAE: 0.006653767544776201 - MAPE: -6.977448463439941 - RMSE: 0.009122492745518684 \n","Epoch number: 39 - Training Loss: 0.010934950314266285 - MAE: 0.0066553084179759026 - MAPE: 0.5680522918701172 - RMSE: 0.009124698117375374 \n","Epoch number: 40 - Training Loss: 0.010923851469835 - MAE: 0.0066464669071137905 - MAPE: -0.12719333171844482 - RMSE: 0.00911510456353426 \n","Epoch number: 41 - Training Loss: 0.010931549568305197 - MAE: 0.006652770098298788 - MAPE: -0.900036096572876 - RMSE: 0.009123173542320728 \n","Epoch number: 42 - Training Loss: 0.010929523051355264 - MAE: 0.006653637159615755 - MAPE: -2.651580333709717 - RMSE: 0.009121413342654705 \n","Epoch number: 43 - Training Loss: 0.010927001218680506 - MAE: 0.006650570780038834 - MAPE: -0.9420304298400879 - RMSE: 0.009115537628531456 \n","Epoch number: 44 - Training Loss: 0.010930407052051845 - MAE: 0.006654786877334118 - MAPE: -1.5057483911514282 - RMSE: 0.009122329764068127 \n","Epoch number: 45 - Training Loss: 0.010929417765863697 - MAE: 0.00665241340175271 - MAPE: -1.0367043018341064 - RMSE: 0.009120008908212185 \n","Epoch number: 46 - Training Loss: 0.010933580469305301 - MAE: 0.006653346586972475 - MAPE: -0.5436753630638123 - RMSE: 0.009120221249759197 \n","Epoch number: 47 - Training Loss: 0.010923656673238007 - MAE: 0.006649849936366081 - MAPE: -0.253409206867218 - RMSE: 0.009115776047110558 \n","Epoch number: 48 - Training Loss: 0.01093073431934873 - MAE: 0.006651213392615318 - MAPE: -1.1654390096664429 - RMSE: 0.00912005640566349 \n","Epoch number: 49 - Training Loss: 0.010934347780516872 - MAE: 0.00665497500449419 - MAPE: -1.226193904876709 - RMSE: 0.009123660624027252 \n","Epoch number: 50 - Training Loss: 0.010930303507888619 - MAE: 0.006653188727796078 - MAPE: -0.5000279545783997 - RMSE: 0.009122123010456562 \n","Epoch number: 51 - Training Loss: 0.010933781944408636 - MAE: 0.006654498632997274 - MAPE: -0.4677000045776367 - RMSE: 0.009124836884438992 \n","Epoch number: 52 - Training Loss: 0.010938131495795518 - MAE: 0.006655663251876831 - MAPE: 3.2746975421905518 - RMSE: 0.009125533513724804 \n","Epoch number: 53 - Training Loss: 0.010931357953900385 - MAE: 0.006653175689280033 - MAPE: -1.2647653818130493 - RMSE: 0.009124797768890858 \n","Epoch number: 54 - Training Loss: 0.010936964491685587 - MAE: 0.0066540916450321674 - MAPE: -0.45840805768966675 - RMSE: 0.009124028496444225 \n","Epoch number: 55 - Training Loss: 0.01092487404612377 - MAE: 0.00665082223713398 - MAPE: -4.239317893981934 - RMSE: 0.009118128567934036 \n","Epoch number: 56 - Training Loss: 0.010934196899537745 - MAE: 0.006656206678599119 - MAPE: 0.8827764391899109 - RMSE: 0.009123913012444973 \n","Epoch number: 57 - Training Loss: 0.010917512399603863 - MAE: 0.006650872528553009 - MAPE: -0.5544121265411377 - RMSE: 0.009116957895457745 \n","Epoch number: 58 - Training Loss: 0.010935336677466921 - MAE: 0.006655373144894838 - MAPE: 0.5794326066970825 - RMSE: 0.009123365394771099 \n","Epoch number: 59 - Training Loss: 0.010924263481777777 - MAE: 0.006648340728133917 - MAPE: 56.74830627441406 - RMSE: 0.009116736240684986 \n","Epoch number: 60 - Training Loss: 0.010925625501004337 - MAE: 0.006651720497757196 - MAPE: 2.680452823638916 - RMSE: 0.009118849411606789 \n","Epoch number: 61 - Training Loss: 0.010922359089988773 - MAE: 0.006650600582361221 - MAPE: 0.17861078679561615 - RMSE: 0.009118703193962574 \n","Epoch number: 62 - Training Loss: 0.010931939106869275 - MAE: 0.006654899567365646 - MAPE: -3.313215494155884 - RMSE: 0.009123473428189754 \n","Epoch number: 63 - Training Loss: 0.010934672821644977 - MAE: 0.006656568497419357 - MAPE: -1.4741559028625488 - RMSE: 0.009126297198235989 \n","Epoch number: 64 - Training Loss: 0.010931678303685079 - MAE: 0.006651460193097591 - MAPE: 0.4165771007537842 - RMSE: 0.009120257571339607 \n","Epoch number: 65 - Training Loss: 0.010930366557632012 - MAE: 0.00665081525221467 - MAPE: -4.037270545959473 - RMSE: 0.009120018221437931 \n","Epoch number: 66 - Training Loss: 0.010926898200330151 - MAE: 0.006649158429354429 - MAPE: 0.5763183832168579 - RMSE: 0.009117824025452137 \n","Epoch number: 67 - Training Loss: 0.01093064453509376 - MAE: 0.006653092335909605 - MAPE: -1.9636378288269043 - RMSE: 0.009123582392930984 \n","Epoch number: 68 - Training Loss: 0.01093105038068559 - MAE: 0.006653905846178532 - MAPE: -1.290953278541565 - RMSE: 0.009122217074036598 \n","Epoch number: 69 - Training Loss: 0.010926299156066157 - MAE: 0.006656117271631956 - MAPE: -0.3064182698726654 - RMSE: 0.009123994037508965 \n","Epoch number: 70 - Training Loss: 0.010935749508858329 - MAE: 0.0066544171422719955 - MAPE: -1.3797234296798706 - RMSE: 0.009125852026045322 \n","Epoch number: 71 - Training Loss: 0.010931296761573887 - MAE: 0.006655647419393063 - MAPE: -2.9732632637023926 - RMSE: 0.009122651070356369 \n","Epoch number: 72 - Training Loss: 0.010929761476763104 - MAE: 0.006652685347944498 - MAPE: -1.9231253862380981 - RMSE: 0.009121053852140903 \n","Epoch number: 73 - Training Loss: 0.010924564396972446 - MAE: 0.006648653652518988 - MAPE: -1.4891400337219238 - RMSE: 0.00911739468574524 \n","Epoch number: 74 - Training Loss: 0.010929407823220312 - MAE: 0.006652638781815767 - MAPE: -0.34187331795692444 - RMSE: 0.009120953269302845 \n","Epoch number: 75 - Training Loss: 0.01093253916178658 - MAE: 0.006653917953372002 - MAPE: -0.1276734173297882 - RMSE: 0.009121883660554886 \n","Epoch number: 76 - Training Loss: 0.010928019677239909 - MAE: 0.0066553098149597645 - MAPE: -0.5349942445755005 - RMSE: 0.009124546311795712 \n","Epoch number: 77 - Training Loss: 0.010924036938375082 - MAE: 0.006648601498454809 - MAPE: 0.3237355649471283 - RMSE: 0.009115542285144329 \n","Epoch number: 78 - Training Loss: 0.010932333425550368 - MAE: 0.006655600853264332 - MAPE: -0.8568377494812012 - RMSE: 0.009126140736043453 \n","Epoch number: 79 - Training Loss: 0.010927974498305402 - MAE: 0.0066556972451508045 - MAPE: -1.4974089860916138 - RMSE: 0.009122694842517376 \n","Epoch number: 80 - Training Loss: 0.0109255473665774 - MAE: 0.006653326563537121 - MAPE: -1.1619503498077393 - RMSE: 0.009118540212512016 \n","Epoch number: 81 - Training Loss: 0.010933038902534012 - MAE: 0.006655416451394558 - MAPE: 0.6636020541191101 - RMSE: 0.00912381336092949 \n","Epoch number: 82 - Training Loss: 0.01092677570641734 - MAE: 0.006651957053691149 - MAPE: 0.29220741987228394 - RMSE: 0.009118570014834404 \n","Epoch number: 83 - Training Loss: 0.010932217316508137 - MAE: 0.006654202006757259 - MAPE: 0.7061201930046082 - RMSE: 0.009122290648519993 \n","Epoch number: 84 - Training Loss: 0.010929744623436266 - MAE: 0.006652893964201212 - MAPE: -0.5046930313110352 - RMSE: 0.009122475050389767 \n","Epoch number: 85 - Training Loss: 0.010928980508074356 - MAE: 0.006650637369602919 - MAPE: -0.8937742710113525 - RMSE: 0.009118324145674706 \n","Epoch number: 86 - Training Loss: 0.010928644479505862 - MAE: 0.006651110481470823 - MAPE: 0.6869775056838989 - RMSE: 0.00911847222596407 \n","Epoch number: 87 - Training Loss: 0.010930418107779621 - MAE: 0.006652404088526964 - MAPE: -0.19189658761024475 - RMSE: 0.009122774004936218 \n","Epoch number: 88 - Training Loss: 0.01092296703209915 - MAE: 0.006646900437772274 - MAPE: -9.148504257202148 - RMSE: 0.009113477542996407 \n","Epoch number: 89 - Training Loss: 0.010927061898851611 - MAE: 0.006650737952440977 - MAPE: 0.146712064743042 - RMSE: 0.00911958422511816 \n","Epoch number: 90 - Training Loss: 0.010932030168098362 - MAE: 0.006651764735579491 - MAPE: -1.5081855058670044 - RMSE: 0.009120832197368145 \n","Epoch number: 91 - Training Loss: 0.01092767195787095 - MAE: 0.006650175433605909 - MAPE: 0.5899088382720947 - RMSE: 0.009117068722844124 \n","Epoch number: 92 - Training Loss: 0.010928469848173921 - MAE: 0.006652576383203268 - MAPE: 0.8209760189056396 - RMSE: 0.009121578186750412 \n","Epoch number: 93 - Training Loss: 0.010928823051019968 - MAE: 0.00665342528373003 - MAPE: -1.1802653074264526 - RMSE: 0.009121757000684738 \n","Epoch number: 94 - Training Loss: 0.010936419995385442 - MAE: 0.006652469281107187 - MAPE: 2.0094637870788574 - RMSE: 0.009121863171458244 \n","Epoch number: 95 - Training Loss: 0.010932088321636188 - MAE: 0.006652405951172113 - MAPE: -1.3557403087615967 - RMSE: 0.009122192859649658 \n","Epoch number: 96 - Training Loss: 0.010940124913849507 - MAE: 0.006654806900769472 - MAPE: 4.122421741485596 - RMSE: 0.009127011522650719 \n","Epoch number: 97 - Training Loss: 0.010926891685713537 - MAE: 0.0066489665769040585 - MAPE: -0.6787999272346497 - RMSE: 0.009119677357375622 \n","Epoch number: 98 - Training Loss: 0.010926523254780437 - MAE: 0.006650836672633886 - MAPE: -0.5492857098579407 - RMSE: 0.009118298999965191 \n","Epoch number: 99 - Training Loss: 0.010934874515267945 - MAE: 0.006654843222349882 - MAPE: -2.72874116897583 - RMSE: 0.009123182855546474 \n","Epoch number: 100 - Training Loss: 0.010926870994635612 - MAE: 0.006648954935371876 - MAPE: -0.49405503273010254 - RMSE: 0.009117520414292812 \n"]}]},{"cell_type":"code","source":["best_model = BasicGAT(in_features, hidden_channels, num_layers, out_features).to(device)\n","best_model.load_state_dict(torch.load(save_path))"],"metadata":{"id":"FZm6B2tWDlXV","executionInfo":{"status":"ok","timestamp":1719245290773,"user_tz":-120,"elapsed":19,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc5f1af7-2bac-4f82-92f3-612b57fe5b00"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":[],"metadata":{"id":"2ZqL9SmwUy9H","executionInfo":{"status":"ok","timestamp":1719245290773,"user_tz":-120,"elapsed":18,"user":{"displayName":"Methembe Thomas Tshuma","userId":"01504251177160369276"}}},"execution_count":7,"outputs":[]}]}